# -*- coding: utf-8 -*-
"""Untitled15.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1z7LBB7E0FJevRJwNpgrwzoGC7FN3Y-vT
"""

!pip install torch torchvision torchaudio scikit-learn matplotlib

from google.colab import files
uploaded = files.upload()

import pandas as pd

# Read the uploaded CSV file into a pandas DataFrame
file_name = "Phenotypic_V1_0b_preprocessed1.csv"
df = pd.read_csv(file_name)

# Check the first few rows of the dataset to understand its structure
df.head()

# Check the columns and basic information about the dataset
df.info()

# Check for missing values in the dataset
df.isnull().sum()

# Check summary statistics for numeric columns
df.describe()

# Replace -9999 with NaN
df.replace(-9999, pd.NA, inplace=True)

# Check for missing values again
df.isnull().sum()

# Drop rows with missing values
df.dropna(inplace=True)

# Check the shape of the DataFrame after dropping rows with missing values
df.shape

# Fill missing values with 0 (or another value)
df.fillna(0, inplace=True)

import matplotlib.pyplot as plt
import seaborn as sns

# Plot the distribution of AGE_AT_SCAN
plt.figure(figsize=(10, 6))
sns.histplot(df['AGE_AT_SCAN'], kde=True)
plt.title('Distribution of Age at Scan')
plt.xlabel('Age at Scan')
plt.ylabel('Frequency')
plt.show()

# Plot the distribution of FIQ
plt.figure(figsize=(10, 6))
sns.histplot(df['FIQ'], kde=True)
plt.title('Distribution of FIQ')
plt.xlabel('FIQ')
plt.ylabel('Frequency')
plt.show()

# Correlation matrix for numeric columns
correlation_matrix = df.corr()

# Plot the correlation matrix using a heatmap
plt.figure(figsize=(12, 8))
sns.heatmap(correlation_matrix, annot=True, cmap="coolwarm", fmt=".2f")
plt.title('Correlation Matrix')
plt.show()

# Convert categorical column 'SEX' to numerical (if applicable)
df['SEX'] = df['SEX'].map({'M': 1, 'F': 0})  # Assuming 'M' and 'F' are the categories

# Check the data again
df.head()

# Save the cleaned DataFrame to a new CSV file
df.to_csv("Cleaned_Phenotypic_Data.csv", index=False)

# Display the first few rows of the dataset to inspect data
print(df.head())

import os
print(os.listdir())  # Lists all files in the current directory

import pandas as pd

# Read the dataset again
df = pd.read_csv("Phenotypic_V1_0b_preprocessed1.csv")

# Display the first few rows to inspect the data
print(df.head())
print(f"Shape of the dataset: {df.shape}")

# Drop irrelevant columns
df = df.drop(columns=['Unnamed: 0.1', 'Unnamed: 0'])

# Drop rows with missing target variable (if applicable)
df = df.dropna(subset=['DX_GROUP'])

# Select numeric columns
numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns

# Fill missing values for numeric columns with their mean
df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].mean())

# Select non-numeric columns (categorical columns)
categorical_cols = df.select_dtypes(include=['object']).columns

# Fill missing values for categorical columns with the mode
for col in categorical_cols:
    df[col] = df[col].fillna(df[col].mode()[0])

# Select numeric columns
numeric_cols = df.select_dtypes(include=['float64', 'int64']).columns

# Fill missing values for numeric columns with their mean
df[numeric_cols] = df[numeric_cols].fillna(df[numeric_cols].mean())

# Select non-numeric columns (categorical columns)
categorical_cols = df.select_dtypes(include=['object']).columns

# Fill missing values for categorical columns with the mode
for col in categorical_cols:
    df[col] = df[col].fillna(df[col].mode()[0])

# Confirm the missing values are handled
print(df.isnull().sum())

# Encoding categorical columns using one-hot encoding
df = pd.get_dummies(df, drop_first=True)

from sklearn.preprocessing import StandardScaler

# Separate features (X) and target (y)
X = df.drop(columns=['DX_GROUP'])  # Assuming 'DX_GROUP' is the target column
y = df['DX_GROUP']  # This is the target variable

# Initialize the StandardScaler
scaler = StandardScaler()

# Fit and transform the features
X_scaled = scaler.fit_transform(X)

# Now X_scaled is ready for the model training

from sklearn.model_selection import train_test_split

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# Print shapes of training and testing data
print(f"Training data shape: {X_train.shape}")
print(f"Testing data shape: {X_test.shape}")

from sklearn.model_selection import cross_val_score

# Perform k-fold cross-validation with Random Forest
cv_scores = cross_val_score(rf_model, X_train, y_train, cv=5, scoring='accuracy')

print(f"Average Cross-Validation Accuracy: {cv_scores.mean() * 100:.2f}%")

from sklearn.model_selection import GridSearchCV

# Set up the parameter grid
param_grid = {
    'n_estimators': [100, 200, 300],
    'max_depth': [None, 10, 20, 30],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4]
}

# Initialize RandomForestClassifier
rf = RandomForestClassifier(random_state=42)

# Initialize GridSearchCV
grid_search = GridSearchCV(estimator=rf, param_grid=param_grid, cv=5, n_jobs=-1, verbose=2)

# Fit the model to find the best parameters
grid_search.fit(X_train, y_train)

# Print the best parameters and best score
print(f"Best parameters: {grid_search.best_params_}")
print(f"Best cross-validation accuracy: {grid_search.best_score_}")

from imblearn.over_sampling import SMOTE
smote = SMOTE(random_state=42)
X_res, y_res = smote.fit_resample(X, y)

from sklearn.ensemble import RandomForestClassifier

model = RandomForestClassifier(
    n_estimators=200,
    max_depth=10,  # Set a limit to depth
    min_samples_split=10,  # Minimum samples for a split
    min_samples_leaf=4,  # Minimum samples for a leaf
    random_state=42
)



from sklearn.feature_selection import SelectFromModel

selector = SelectFromModel(model, threshold="mean", max_features=10)
X_new = selector.fit_transform(X, y)

model = RandomForestClassifier(
    n_estimators=200,
    max_depth=10,
    min_samples_split=10,
    min_samples_leaf=4,
    random_state=42,
    class_weight='balanced'  # Use balanced class weights
)

from sklearn.model_selection import StratifiedKFold, cross_val_score

stratified_kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)
scores = cross_val_score(model, X, y, cv=stratified_kf)
print("Stratified Cross-validation accuracy scores:", scores)
print("Average stratified cross-validation accuracy:", scores.mean())

import joblib

# Save the trained model
joblib.dump(grid_search.best_estimator_, 'cross_val_model.pkl')

# To load the model later
model = joblib.load('cross_val_model.pkl')